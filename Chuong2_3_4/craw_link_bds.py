import csv
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin
# Thay tháº¿ webdriver tiÃªu chuáº©n báº±ng undetected_chromedriver
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def create_driver():
    """Táº¡o Chrome driver sá»­ dá»¥ng undetected-chromedriver Ä‘á»ƒ trÃ¡nh bá»‹ phÃ¡t hiá»‡n"""
    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument('--incognito')
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    # === THÃŠM DÃ’NG NÃ€Y VÃ€O ===
    # Buá»™c thÆ° viá»‡n táº£i driver cho phiÃªn báº£n 128
    driver = uc.Chrome(options=chrome_options, version_main=128) 
    
    return driver

def crawl_bds_links(base_url, max_pages=10):
    """
    Crawl links tá»« batdongsan.com.vn sá»­ dá»¥ng undetected_chromedriver
    
    Args:
        base_url: URL gá»‘c
        max_pages: Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl
    """
    all_links = []
    driver = None
    
    try:
        print("ğŸš€ Khá»Ÿi táº¡o undetected-chromedriver...")
        driver = create_driver()
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}/p{page}" if page > 1 else base_url
            
            print(f"ğŸ“„ Äang crawl trang {page}: {url}")
            
            try:
                driver.get(url)
                
                # Chá» cho trang vÆ°á»£t qua kiá»ƒm tra cá»§a Cloudflare vÃ  load ná»™i dung chÃ­nh
                # TÄƒng thá»i gian chá» vÃ  chá» má»™t element cá»¥ thá»ƒ cá»§a trang sáº£n pháº©m
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.re__srp-list"))
                )
                
                # Chá» thÃªm má»™t chÃºt Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ JavaScript Ä‘Ã£ cháº¡y
                time.sleep(random.uniform(2, 4))
                
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                title = soup.find('title').get_text(strip=True) if soup.find('title') else "KhÃ´ng cÃ³ title"
                print(f"ğŸ” Trang {page} title: {title[:70]}...")

                if "Just a moment" in title:
                    print(f"âŒ Váº«n bá»‹ Cloudflare cháº·n á»Ÿ trang {page}. Thá»­ láº¡i hoáº·c dá»«ng láº¡i.")
                    continue

                product_links = soup.select('a.js__product-link-for-product-id')
                
                if not product_links:
                    print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y link sáº£n pháº©m nÃ o á»Ÿ trang {page}.")
                    continue
                
                page_links = []
                for link in product_links:
                    href = link.get('href')
                    if href:
                        # urljoin Ä‘áº£m báº£o link Ä‘Æ°á»£c táº¡o chÃ­nh xÃ¡c
                        full_url = urljoin('https://batdongsan.com.vn', href)
                        page_links.append(full_url)
                
                all_links.extend(page_links)
                print(f"âœ… TÃ¬m tháº¥y {len(page_links)} links á»Ÿ trang {page}")
                
                # Delay ngáº«u nhiÃªn dÃ i hÆ¡n má»™t chÃºt Ä‘á»ƒ hÃ nh xá»­ giá»‘ng ngÆ°á»i hÆ¡n
                delay = random.uniform(3, 6)
                print(f"ğŸ˜´ Táº¡m nghá»‰ {delay:.2f} giÃ¢y...")
                time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ Lá»—i khi crawl trang {page}: {e}")
                # Chá»¥p áº£nh mÃ n hÃ¬nh Ä‘á»ƒ debug náº¿u cáº§n
                driver.save_screenshot(f'error_page_{page}.png')
                print(f"ğŸ“¸ ÄÃ£ lÆ°u áº£nh mÃ n hÃ¬nh lá»—i vÃ o 'error_page_{page}.png'")
                continue
                
    except Exception as e:
        print(f"âŒ Lá»—i nghiÃªm trá»ng: {e}")
        
    finally:
        if driver:
            driver.quit()
            print("ğŸ”š ÄÃ£ Ä‘Ã³ng Chrome driver")
    
    return all_links

def save_links_to_csv(links, filename='link_bds.csv'):
    """LÆ°u danh sÃ¡ch links vÃ o file CSV"""
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['STT', 'Link'])
            
            for i, link in enumerate(links, 1):
                writer.writerow([i, link])
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(links)} links vÃ o file {filename}")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file CSV: {e}")

def main():
    base_url = "https://batdongsan.com.vn/nha-dat-ban-ha-noi"
    
    print("ğŸ  Báº¯t Ä‘áº§u crawl links tá»« batdongsan.com.vn")
    print("=" * 50)
    
    links = crawl_bds_links(base_url, max_pages=100)
    
    if links:
        unique_links = sorted(list(set(links)))
        print(f"\nğŸ“Š Tá»•ng cá»™ng: {len(links)} links")
        print(f"ğŸ“Š Sau khi loáº¡i bá» duplicate: {len(unique_links)} links")
        
        save_links_to_csv(unique_links, 'link_bds.csv')
        
        print("\nğŸ”— Má»™t vÃ i link máº«u:")
        for i, link in enumerate(unique_links[:5], 1):
            print(f"{i}. {link}")
            
    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y link nÃ o!")

if __name__ == "__main__":
    main()